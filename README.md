# Heterogeneous (UGV-UAV) Coordination for Persistent Monitoring using Particle Filters and Reinforcement Learning
 

In this project, we study the multi-robot visibility-based persistent monitoring (PM) problem. Persistent monitoring is a variant of coverage problem that involves repeatedly covering a given area, e.g. patrolling, security surveillance, target searching, etc. In addition to the coverage maximization objective of the well-studied exploration problem, the PM problem also requires agents to revisit previously explored regions in order to achieve repeated monitoring of the space. More specifically, in this work, we study a novel visibility-based PM problem where a team of UGVs equipped with 360◦ visual sensors repeatedly patrols an obstructed 2D region. We call the problem visibility-based persistent monitoring (VPM) problem. 


###

We propose novel algorithms to plan the path of multiple UGVs to solve the VPM problem. The design of an algorithm that plans the motion of the UGVs performing the VPM task depends on whether each UGV knows the position of all the other UGVs. In a practical scenario, it might be unrealistic to assume that each UGV always knows the positions of all other UGVs exactly, which requires uninterrupted communication among all the UGVs. In this work, we make a more realistic assumption that the UGVs might not always maintain communication with one another, hence have incomplete information about the position of the other UGVs.

Our solution to the VPM problem uses the idea of [Particle Filters](https://en.wikipedia.org/wiki/Particle_filter) to model the uncertainty in the position of other robots. We apply [Rapidly-exploring Random Tree (RRT)](https://en.wikipedia.org/wiki/Rapidly_exploring_random_tree) and Receding Horizon strategy to plan the paths of the UGVs. We conduct simulation-based experiments to demonstrate the effectiveness of our proposed algorithm.

<p align="center">
  <img src="/images/hcpf_plot.png" width="400"/>
</p> 


We evaluate the performance of our UGV planning algorithm (VPM) against two algorithms. (1) The VPMT algorithm, in which each UGV knows the true location of all other UGVs. (2) The VPM-O algorithm, in which the UGVs neither know the position of the other UGVs, nor maintain a position belief. The experimental results are shown in the figure above.



### UAV Planning

In our formulation of heterogeneous coordination, since a UGV does know the exact position of other UGVs, we use the UAV to sample and update the belief about all UGVs’ positions and use this shared information to assist the path planning of all UGVs. Thus UAV needs to plan a path that minimizes its uncertainty about UGVs’ positions and share this information with them.

The UAV planning problem is modeled using a Markov Decision Process (MDP), where the state is the heatmap generated by a particle filter-based algorithm, the set of actions is the four principal directions, the transition function is deterministic, and the reward function is given by the sum of latency values of all the cells. 

<p align="center">
  <img src="/images/hcrl_plot.png" width="500"/>
</p> 

We solve the above MDP using a Reinforcement Learning-based approach. We train an Actor-Critic version of [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) to investigate how the learning-based approach performs on the UAV path planning problem. The PPO takes
as input the heatmap and outputs the action for the UAV. Our experimental findings suggest that the UAV effectively learns a policy over time as evidenced by the figure above.